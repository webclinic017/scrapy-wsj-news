{
    "url_original": "https://www.wsj.com/articles/artificial-intelligence-ai-gpt-3-turing-test-reason-science-technology-11636144673?mod=opinion_major_pos18",
    "url": "artificial-intelligence-ai-gpt-3-turing-test-reason-science-technology-11636144673",
    "title": "Artificial Intelligence Is Still No Match for Us",
    "sub_head": "Scientists misunderstand human reason and an AI system fails the Turing test.",
    "category_1": "Opinion",
    "category_2": "Letters",
    "image_1_url": "https://images.wsj.net/im-427090?width=860&height=573",
    "image_1": "im-427090.jpg",
    "time": "2021-11-07 13:19:00",
    "body": "Eric Schmidt was executive chairman while I was in the trenches at Google in 2012, but I know better than to claim—as he does with Henry Kissinger and Daniel Huttenlocher—that GPT-3 is “producing original text that meets Alan Turing’s standard.” The GPT-3 program hasn’t passed the Turing test, and it seems nowhere near doing so (“The Challenge of Being Human in the Age of AI,” op-ed, Nov. 2).<br />Compared with earlier text-generation systems, the output generated by GPT-3 looks impressive at a local level; individual phrases, sentences and paragraphs usually demonstrate good grammar and look like normal human-generated text. But at a global level—considering the meaning of multiple sentences, paragraphs or a back-and-forth dialogue—it becomes apparent that GPT-3 doesn’t understand what it’s talking about. It doesn’t have common-sense reasoning or the ability to keep track of objects over time in a discussion. One example, published in August 2020 in MIT Technology Review: GPT-3 was asked, “Yesterday I dropped my clothes off at the dry cleaner’s and I have yet to pick them up. Where are my clothes?” Its response: “I have a lot of clothes.”"
}